{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Window Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to define a NLP neural network able to detect color-related words contained in a sentence.\\\n",
    "Inspired by [Standford's CS224N lab](https://stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present dataset is composed by different sentences.\\\n",
    "The purpose of the model is to be able to recognize a color among these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The sky was a brilliant shade of blue.\",\n",
    "    \"She wore a stunning red dress to the party.\",\n",
    "    \"The leaves turned vibrant shades of orange and yellow in the fall.\",\n",
    "    \"His new car is a sleek black.\",\n",
    "    \"The walls of the room were painted a calming light green.\",\n",
    "    \"A fluffy white cat sat on the windowsill.\",\n",
    "    \"The sunset was a beautiful mix of pink and purple.\",\n",
    "    \"She admired the emerald green of the gemstone.\",\n",
    "    \"The artist used a lot of bright yellow in his painting.\",\n",
    "    \"The ocean looked turquoise under the midday sun.\",\n",
    "    \"He bought a pair of brown leather shoes.\",\n",
    "    \"The morning sky was a soft pastel pink.\",\n",
    "    \"The old book had a faded blue cover.\",\n",
    "    \"Her eyes were a striking shade of hazel.\",\n",
    "    \"The banana was ripe and yellow.\",\n",
    "    \"The house had a cheerful red front door.\",\n",
    "    \"The mountain peaks were capped with white snow.\",\n",
    "    \"She chose a light grey suit for the interview.\",\n",
    "    \"The grapes were a dark purple, almost black.\",\n",
    "    \"The sunset painted the horizon with orange and red hues.\",\n",
    "    \"The bird had bright blue feathers.\",\n",
    "    \"The grass was lush and green after the rain.\",\n",
    "    \"He preferred writing with a black pen.\",\n",
    "    \"Her hair was dyed a vivid purple.\",\n",
    "    \"The butterfly had wings of brilliant blue.\",\n",
    "    \"The apples were shiny and red.\",\n",
    "    \"The night sky was a deep, inky black.\",\n",
    "    \"She decorated the room with light lavender curtains.\",\n",
    "    \"Who are these greens snakes that are whistling on your heads for?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, when starting a NLP task, it is recommanded to preprocess the given sentences this way:\\\n",
    "\\- lowering each character;\\\n",
    "\\- splitting the sentence into tokens (i.e. an array where each element is a word);\\\n",
    "\\- removing stop words;\\\n",
    "\\- stemming or lemmatizing the words.\n",
    "\n",
    "With regards to the fact that this notebook is mainly made to expose word window classifiers, the two last steps are not going to be performed as they are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    for punc in set([',', ';', '.', '?', '!', '/', \"'\", '-', '_']): sentence = sentence.replace(punc, '')\n",
    "    return sentence.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Example \n",
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sentence `\"The sky was a brilliant shade of blue.\"` is transfored into the array of tokens `['the', 'sky', 'was', 'a', 'brilliant', 'shade', 'of', 'blue']`, which is actually better when it comes to machine learning tasks.\n",
    "\n",
    "The following array defines the labels of the input words. For sentence, the array contains the name number of integers as the input array has words. These integers are equal to `1`if the word refers to a color, else `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1],              # \"The sky was a brilliant shade of blue.\"\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0],           # \"She wore a stunning red dress to the party.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],  # \"The leaves turned vibrant shades of orange and yellow in the fall.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1],                 # \"His new car is a sleek black.\"\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],     # \"The walls of the room were painted a calming light green.\"\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],              # \"A fluffy white cat sat on the windowsill.\"\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],        # \"The sunset was a beautiful mix of pink and purple.\"\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0],              # \"She admired the emerald green of the gemstone.\"\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],     # \"The artist used a lot of bright yellow in his painting.\"\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],              # \"The ocean looked turquoise under the midday sun.\"\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],              # \"He bought a pair of brown leather shoes.\"\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1],              # \"The morning sky was a soft pastel pink.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0],              # \"The old book had a faded blue cover.\"\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1],              # \"Her eyes were a striking shade of hazel.\"\n",
    "    [0, 0, 0, 0, 0, 1],                    # \"The banana was ripe and yellow.\"\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],              # \"The house had a cheerful red front door.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0],              # \"The mountain peaks were capped with white snow.\"\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0],           # \"She chose a light grey suit for the interview.\"\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1],              # \"The grapes were a dark purple, almost black.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],        # \"The sunset painted the horizon with orange and red hues.\"\n",
    "    [0, 0, 0, 0, 1, 0],                    # \"The bird had bright blue feathers.\"\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0],           # \"The grass was lush and green after the rain.\"\n",
    "    [0, 0, 0, 0, 0, 1, 0],                 # \"He preferred writing with a black pen.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1],                 # \"Her hair was dyed a vivid purple.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1],                 # \"The butterfly had wings of brilliant blue.\"\n",
    "    [0, 0, 0, 0, 0, 1],                    # \"The apples were shiny and red.\"\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1],              # \"The night sky was a deep, inky black.\"\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0],              # \"She decorated the room with light lavender curtains.\"\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   # \"Who are these greens snakes that are whistling on your heads for?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient for the following tasks to define a set of words, here called `vocabulary`.\\\n",
    "This set contains every word contained in the dataset's sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(word for sentence in train_sentences for word in sentence)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asking the model's to predict what is the color(s) of a sentence, it is possible that it faces a word that is not in its vocabulary.\\\n",
    "To prevent from this, the unknown word is added to the set of words as `'<unk>'`.\\\n",
    "Also, as the models relies on a window of words sliding through the sentence, it is required to define another special words to handle the side effects. This padding word is defined as `'<pad>'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.add('<unk>')\n",
    "vocabulary.add('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_window(sentence: List[str], window_size: int) -> List[str]:\n",
    "    window = ['<pad>'] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "# Example\n",
    "print(f'Padded sentence: {pad_window(train_sentences[0], window_size=2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to automatically add padding to each tokenized sentences.\\\n",
    "The next step to make this dataset compatible with machine learning models is to give to each word a unique index it can refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: index for index, word in enumerate(sorted(list(vocabulary)))}\n",
    "idx_to_word = {index: word for index, word in enumerate(sorted(list(vocabulary)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions are now defined to process this way the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(sentence: List[str], word_to_idx: Dict[str, int]) -> List[int]:\n",
    "    return [word_to_idx[token] if token in word_to_idx else word_to_idx['<unk>'] for token in sentence]\n",
    "\n",
    "def indices_to_token(indices: List[int], idx_to_word: Dict[int, str]) -> List[str]:\n",
    "    return [idx_to_word[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ver = 'Are birds red?'\n",
    "tokenized_ver = tokenize(sentence_ver)\n",
    "indices_ver = tokens_to_indices(tokenized_ver, word_to_idx)\n",
    "restored_ver = indices_to_token(indices_ver, idx_to_word)\n",
    "\n",
    "print(f'Original: {sentence_ver}')\n",
    "print(f'Tokenized: {tokenized_ver}')\n",
    "print(f'Indices: {indices_ver}')\n",
    "print(f'Restored: {restored_ver}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, \"birds\" is not a word of the set of words previously defined. Thus, is it replaced by `'<unk>'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [tokens_to_indices(sentence, word_to_idx) for sentence in train_sentences]\n",
    "train_indices[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.Embedding(len(vocabulary), embedding_dim=5)\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_fn(batch, window_size: int, word_to_idx: Dict[str, int]) -> any:\n",
    "    X, y = zip(*batch)\n",
    "    X = [pad_window(s, window_size=window_size) for s in X]\n",
    "    X = [tokens_to_indices(s, word_to_idx) for s in X]\n",
    "    X = [torch.LongTensor(X_i) for X_i in X]\n",
    "\n",
    "    pad_token_index = word_to_idx['<pad>']\n",
    "    X_padded = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=pad_token_index)\n",
    "\n",
    "    lengths = torch.LongTensor([len(label) for label in y])\n",
    "\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    return X_padded, y_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(train_sentences, train_labels))\n",
    "collate_fn = partial(_collate_fn, window_size=2, word_to_idx=word_to_idx)\n",
    "\n",
    "loader = DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for counter, (batched_X, batched_y, batched_lengths) in enumerate(loader):\n",
    "    print(f\"Iteration {counter}\")\n",
    "    print(\"Batched Input:\")\n",
    "    print(batched_X)\n",
    "    print(\"Batched Labels:\")\n",
    "    print(batched_y)\n",
    "    print(\"Batched Lengths:\")\n",
    "    print(batched_lengths)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Tensor:')\n",
    "print(batched_X)\n",
    "\n",
    "chunk = batched_X.unfold(1, 2 * 2 + 1, 1)  # 2 * window_size + 1\n",
    "print('\\nWindows:')\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "    def __init__(self, hyperparameters, vocab_size, padding_idx=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "        self.window_size = hyperparameters['window_size']\n",
    "        self.embed_dim = hyperparameters['embed_dim']\n",
    "        self.hidden_dim = hyperparameters['hidden_dim']\n",
    "        self.freeze_embeddings = hyperparameters['freeze_embeddings']\n",
    "        self.word_to_idx = hyperparameters['word_to_idx']\n",
    "        self.idx_to_word = hyperparameters['idx_to_word']\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=padding_idx)\n",
    "        if self.freeze_embeddings: self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        # Hidden Layer\n",
    "        full_window_size = 2 * self.window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        # Probabilities\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        B, _ = inputs.size()\n",
    "\n",
    "        # Reshaping\n",
    "        token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        # Embedding\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "        # Reshaping\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        # Layer 1\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        # Layer 2\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        # Softmax Score\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def predict(self, input):\n",
    "        for punc in set([',', ';', '.', '?', '!', '/', \"'\", '-', '_']): input = input.replace(punc, '')\n",
    "        tokens = input.lower().split()\n",
    "        window = ['<pad>'] * self.window_size\n",
    "        padded_tokens = window + tokens + window\n",
    "        tokens_idx = [self.word_to_idx[token] for token in padded_tokens]\n",
    "        output = self.forward(torch.tensor([tokens_idx, tokens_idx]))\n",
    "        mask = output[0] > 0.5\n",
    "        target_index = mask.nonzero(as_tuple=True)[0]\n",
    "        if len(target_index) == 0: return None\n",
    "        pred_tokens = [padded_tokens[idx] for idx in target_index]\n",
    "        pred = ' '.join(token for token in pred_tokens)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_collate_fn, window_size=window_size, word_to_idx=word_to_idx)\n",
    "\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "model_hyperparameters = {\n",
    "    'batch_size': batch_size,\n",
    "    'window_size': window_size,\n",
    "    'embed_dim': 2*32,\n",
    "    'hidden_dim': 100,\n",
    "    'freeze_embeddings': False,\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'idx_to_word': idx_to_word\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(batch_outputs, batch_labels, batch_lengths) -> float:\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "    \n",
    "    # Rescaling\n",
    "    loss = loss/batch_lengths.sum().float()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "        optimizer.zero_grad()  # clear gradient\n",
    "        outputs = model.forward(batch_inputs)  # forward pass\n",
    "        loss = loss_function(outputs, batch_labels, batch_lengths)  # batch loss\n",
    "        loss.backward()  # gradient\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "    for epoch in range(0, num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0: print(f'Epoch: {epoch+1} - Loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"The sky is orange and blue.\",\n",
    "               \"When the sun goes down, the sky turns black.\",\n",
    "               \"The car is purple!\",\n",
    "               \"Is the car violet?\",\n",
    "               \"No color in there.\",\n",
    "               \"I like ham when it's pink.\"]\n",
    "test_sentences = [tokenize(sentence) for sentence in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1, 0, 1],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "               [0, 0, 0, 1],\n",
    "               [0, 0, 0, 1],\n",
    "               [0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "verbose = True\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(_collate_fn, window_size=window_size, word_to_idx=word_to_idx)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for counter, (test_instance, labels, _) in enumerate(test_loader):\n",
    "    print(f'Sample {counter+1}')\n",
    "    outputs = model.forward(test_instance)[0]\n",
    "    if verbose:\n",
    "        print(f'   Labels: {labels}\\n   Outputs: {outputs}')\n",
    "    nb_colors = labels.sum()\n",
    "    if nb_colors == 0:\n",
    "        print('   No color to detect.\\n')\n",
    "        continue\n",
    "    colors_indexes = torch.topk(outputs.flatten(), nb_colors).indices\n",
    "    if nb_colors == 1:\n",
    "        print(f'   Detected color: {idx_to_word[int(test_instance[0, window_size+colors_indexes[0]])]}.\\n')\n",
    "    else:\n",
    "        colors = ', '.join(idx_to_word[int(test_instance[0, window_size+index])] for index in colors_indexes)\n",
    "        print(f'   Detected colors: {colors}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances of the models here are quite good yet it troubles on certain situations.\\\n",
    "To prevent from these, it could be relevant to increase the size of the dataset, look for a word-reduction method (stemming or lemmatizing) or optimize the structure of the neural network as it remains very basic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
